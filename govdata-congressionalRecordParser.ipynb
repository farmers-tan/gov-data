{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congressional Record Monologue and Feature Parser\n",
    "\n",
    "This notebook takes U.S. Congressional Record session text files created from [here](https://github.com/kdunn926/gov-data/blob/master/govdata-congressionalRecordScraper.ipynb) and splits it apart into distinct monolgoes, capitalizing off of the semi-structures format of `Mr. NOBODY: <monologe text>` consistent throughout the GPO's Congressional Record archive.\n",
    "\n",
    "Additionally, it attempts to identify and extract:\n",
    "- session times (start and/or end - work in progess)\n",
    "- mentions within a monologue of other people and proper nouns, both implemented using some hairy regexp\n",
    "- \"context\" of a monologue - each monologue is assigned a unique, sequential id, to facilitate contextual forward/backward exploration of the session when starting from a single monologue\n",
    "\n",
    "We also \"join\" in Congressional \"roster\" data, scraped (and manually cleaned) from Wikipedia - to allow annotating: \n",
    "- party affiliations\n",
    "- state represented\n",
    "- \"role\" in the Congress (whip, speaker, etc.)\n",
    "- term end\n",
    "\n",
    "The output of this notebook is compressed, delimited text files, intended to be loaded in something like Neo4J, or even a SQL database. Neo4J _should_ allow for intuitive queries based on the highly connection nature of this dataset.\n",
    "\n",
    "### Note: this process is rather compute-intensive. It's probably very poorly optimized... some attempt has been made to help this but PRs are always welcome!\n",
    "\n",
    "End to end timing for data from ~1992 to mid-2016 was **9 hours**\n",
    "\n",
    "2012 Macbook, 2.3 GHz Intel Core i7, 8GB 1600 MHz DDR3, no SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import py2neo\n",
    "import glob\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setSpeakers(theRecord, theSpeaker):\n",
    "    proTemporeSections = re.split(\"The SPEAKER pro tempore \\(((?:Mr.|Mrs.|Ms.)\\s?[A-z]{2,}(?:\\s[A-Z][a-z]+[-]?){1,2})\\)\", theRecord)\n",
    "    \n",
    "    replaced = \"\"\n",
    "    name = \"\"\n",
    "    for n, token in enumerate(proTemporeSections):\n",
    "        if re.match(\"^(Mr.|Mrs.|Ms.)\", token):\n",
    "            #print n\n",
    "            name = token\n",
    "        \n",
    "        # This serves double duty, replace references to speaker of the moment with a real name\n",
    "        # and handle the case where speaker of the moment is a woman and subsequent monologues\n",
    "        # reference her as \"Madam Speaker\"\n",
    "        replaced = replaced + re.sub(\"The SPEAKER pro tempore\", name, re.sub(\"Madam Speaker\", \"[ {0} ]\".format(name), token))\n",
    "        \n",
    "    replaced = re.sub(\"The SPEAKER\", theSpeaker, replaced)\n",
    "    \n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monologueRegex = re.compile(\"[^[]\\s((?:Mr.|Mrs.|Ms.|\\^\\^THE\\^\\^)\\s(?:[a-z]{2,}\\s)?[A-Z]{1}[A-z]{2,}(?: of (?:[A-Z][a-z]+[ ]?)+)?)\\.\")\n",
    "def getMonologues(theRecord, theSpeaker):\n",
    "        \n",
    "    with open(theRecord) as theFile:\n",
    "        text = theFile.read()\n",
    "        \n",
    "    ctext = setSpeakers(text, theSpeaker)\n",
    "\n",
    "    # All the newlines have been removed, only operate\n",
    "    # on the first element\n",
    "    ctext = re.sub(\"Mr.\",\" Mr.\", ctext)\n",
    "    ctext = re.sub(\"Mrs.\",\" Mrs.\", ctext)\n",
    "    ctext = re.sub(\"Ms.\",\" Ms.\", ctext)\n",
    "    ctext = re.sub(\"Roll No.\",\" Roll No. \", ctext)\n",
    "    ctext = re.sub(\"The SPEAKER pro tempore\",\" ^^THE^^ SPEAKERPROTEMPORE\", ctext)\n",
    "    ctext = re.sub(\"The SPEAKER\",\" ^^THE^^ SPEAKER\", ctext)\n",
    "    ctext = re.sub(\"The Clerk\",\" ^^THE^^ CLERK.\", ctext)\n",
    "    ctext = re.sub(\"The PRESIDING OFFICER\",\" ^^THE^^ PRESIDINGOFFICER.\", ctext)\n",
    "    ctext = re.sub(\"The assistant legislative clerk read\",\" ^^THE^^ ASSISTANTLEGISLATIVECLERK. read\", ctext)\n",
    "    \n",
    "    #text = re.sub(\"Roll No.\", \" Roll No.\", ctext)\n",
    "    \n",
    "    # Proto to capture roll number, then sub back in with spaces\n",
    "    for rollNumber in re.findall(\"(Roll No.\\s+\\d+)\", ctext):\n",
    "        ctext = re.sub(rollNumber, \" \" + rollNumber + \" \", ctext)\n",
    "    ctext = re.sub(\"[^\\x00-\\x7F]\",' ', ctext)\n",
    "\n",
    "    # TODO capture the roll number and put a space after the roll number\n",
    "\n",
    "    monologues = monologueRegex.split(ctext)\n",
    "    return monologues, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#getMonologues(\"/Users/kyledunn/Desktop/congressionalRecord/HOUSE/Merged/1994-01-26.txt\", \"NoOneInParticular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNameReferences(monologue):\n",
    "    cleanM = monologue.replace(\"]\", \"\")\n",
    "    # Ms. Vaughn or Mr. de Lugo\n",
    "    titleNamePattern = \"(?:Dr|Mr|Mrs|Ms|Madam)[\\.]\\s(?:[A-z]{1}[a-z]{,2}\\s)?(?:Mc|Mac)?(?:[A-Z]{1}[a-z]+)\"\n",
    "    \n",
    "    # Something like Mr. and Mrs. America or Mr. and Mrs. van Daan\n",
    "    couplePattern = \"(?:Dr|Mr|Mrs|Ms)[\\.]\\sand\\s(?:Dr|Mr|Mrs|Ms)[\\.]\\s(?:[A-z]{1}[a-z]{,2}\\s)?(?:Mc|Mac)?(?:[A-Z]{1}[a-z]+)\"\n",
    "    \n",
    "    # Something like John Conyers - this is too wide of a net - moved to Proper Noun extraction\n",
    "    #fullNamePattern = \"(?:[A-Z]{1}[a-z]+){1}\\s(?:[A-z]{1}[a-z]{,2}\\s)?(?:[A-Z]{1}[a-z]+){1}\"\n",
    "    \n",
    "    # Must match the couple pattern first otherwise \"Mrs\" gets interpretted as last name\n",
    "    names = re.findall(\"({0})\".format(\"|\".join([couplePattern, titleNamePattern])), cleanM)\n",
    "    \n",
    "    if names:\n",
    "        return names\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#from stop_words import get_stop_words\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#stopWords.update(get_stop_words('en'))\n",
    "\n",
    "def cleanWord(text):\n",
    "    return re.sub(\"\\W+\", \" \", text).strip()\n",
    "\n",
    "def getSetOfWords(monologue):\n",
    "    return set([cleanWord(word) for word in monologue.split() if (len(cleanWord(word)) > 2) \n",
    "                                                              and cleanWord(word).lower() not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def noStopWordsWithin(phrase):    \n",
    "    for word in phrase.split():\n",
    "        if word.lower() in stopWords:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "# Something like John Conyers - this is too wide of a net - moved to Proper Noun extraction\n",
    "properNounPattern = re.compile(\"(?:(?:Mc|Mac)?(?:[A-Z]{1}[a-z]{2,})\\s(?:[a-z]{2,3}\\s)?){1,}(?:(?:Mc|Mac)?(?:[A-Z]{1}[a-z]{2,}))\")\n",
    "def getProperNounReferences(monologue):\n",
    "    cleanM = monologue.replace(\"]\", \"\")\n",
    "    \n",
    "    refs = [p.strip() for p in properNounPattern.findall(cleanM)]\n",
    "    \n",
    "    if refs:\n",
    "        return refs\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=\"\"\"Mr. Chairman, I yield myself such time as I may consume. Mr. Chairman, the Forest Recovery and Protection Act of \n",
    "1998 is the result of some 14 months of listening and learning and fact-gathering. It is the result of seven hearings in \n",
    "which we heard from a broad array of people across this Nation, including scientists, academics, State foresters, \n",
    "professional associates, environmental groups, wildlife organizations, citizens, community leaders, elected officials, \n",
    "organized labor, the forest products industry and the administration. Beyond the hearing process, the committee has worked \n",
    "exhaustively with minority Members, northeastern Republicans, hopefully all Members of this body to refine the bill to \n",
    "broaden support for what we believe is a very necessary and a very reasonable initiative. We extended a hand and we worked \n",
    "with those who have expressed concerns with the bill and we were willing to work in good faith to find solutions. I am \n",
    "delighted to stand here today and to tell my colleagues that because we have collaborated with these concerned parties \n",
    "we have a stronger bill and one that truly represents, we believe, diverse interests. Here are just a few of the groups, \n",
    "by the way, that support this bill: the AFL-CIO, the United Brotherhood of Carpenters and Joiners of America, the \n",
    "National Association of Counties, the Society of American Foresters, the National Association of State Foresters, the \n",
    "National Association of Professional Forestry Schools. But despite our best efforts to include all interests in crafting \n",
    "this legislation, there are those of course who have elected to remain outside the process rather than coming to the \n",
    "table to seek solutions. Unfortunately, because they have not been engaged, there are some misunderstandings about this \n",
    "bill, which I would like to clear up. There are a number of people who are talking about this bill, about what it is not. \n",
    "I would like to explain to them about what the bill does. It is a five-year pilot project providing a timely and organized \n",
    "and scientific strategy to address the chronic conditions of our national forests. The bill establishes an independent \n",
    "scientific panel through the National Academy of Sciences to recommend to the Secretary of Agriculture the standards and \n",
    "criteria that should be used to identify which national forests are in the worst shape and where restoration efforts are \n",
    "needed most. The public then provides input on the standards and criteria which the Secretary publishes. Based upon the \n",
    "standards and criteria, the Secretary then determines which forests have the greatest restoration needs and allocates \n",
    "amounts to those forests. On-the-ground forest managers then begin planning projects to restore degraded and deteriorating \n",
    "forest resources. I have been hearing information to the contrary, so I want to make this clear to everyone in this assembly. \n",
    "These projects must comply with all applicable environmental laws. This legislation does not in any way limit public \n",
    "participation under existing laws and regulations. More than that, a full, open, public process must be conducted by all \n",
    "recovery projects. All project planning, including analysis of environmental impacts, must comply with NEPA, the \n",
    "National Environmental Policy Act. Recovery projects must be consistent with land and resource management plans, \n",
    "plans that have been analyzed by NEPA and have been deemed consistent with environmental laws and regulations. There is \n",
    "no short-circuiting, circumventing or limiting of laws. Public process or judicial review anywhere in this bill are \n",
    "always protected. So those who oppose 2515, the original bill, must oppose current environmental laws and regulations. \n",
    "Those who oppose this bill must oppose restoring fish habitat. They must oppose reducing the threat of epidemic levels \n",
    "of insects and disease. They must oppose replanting trees and stabilizing slopes after catastrophic events, and they \n",
    "must oppose reducing the risk of wildfire. Those who oppose this bill say the forest health crisis is a myth, that forest \n",
    "health is an excuse to log our national forests. Of course, not every acre in the National Forest is degraded or \n",
    "deteriorating, but over the last decade an enormous body of scientific literature has been generated about our degraded, \n",
    "deteriorating forest resources. Scientists agree that our forests are ``outside the historic range of variability,'' and \n",
    "that active management is necessary in some areas to begin to return forests to their historic conditions. The Chief of \n",
    "the Forest Service has said that there are some 40 million acres of National Forest at unacceptable risk of destruction \n",
    "by catastrophic fire, and listed these sources: the Integrated Scientific Assessment for Ecosystem Management in the \n",
    "Interior Columbia Basin says, ``We found that forests and ecosystems have become more susceptible to severe fire and \n",
    "outbreaks of insects and disease''; the Southern [[Page H1652]] Appalachian Assessment states, ``Several tree species \n",
    "in the Southern Appalachians are at risk of extinction or significant genetic loss because of exotic pests'' and ``lack \n",
    "of active management in other stands has led to development of dense understories, and to the senescence of overstory \n",
    "trees of some species''; the Sierra Nevada Ecosystem Project states, ``Fire protection for the last half century has \n",
    "provided for the development of continuous dense forest stands which are in need of thinning to accelerate growth, reduce \n",
    "fire hazard, provide for more mid-successional forest habitat and yield of usable wood.'' Well, there is no question about \n",
    "it in my mind and all others that this is an essential bill. ``Active management'' is a term that is frequently distorted. \n",
    "Active management could be creating in-stream structure for fish habitat. It could be planting native grasses to stabilize \n",
    "the stream bed; it could be planting trees near a stream to provide shade to reduce stream temperatures; and yes, it could \n",
    "also be cutting trees to prevent the spread of insects and disease or reduce the risk of catastrophic wildfire. It seems to \n",
    "me, Mr. Chairman, that the Forest Service is in some state of catatonic immobilization in that the direction; and the goals \n",
    "of the Forest Service are somehow hidden, and direction is essential, which certainly this legislation does. The \n",
    "Forest Service, I believe, needs emergency care here to help them direct resources in this Nation to protect this very \n",
    "valuable resource. On-the-ground managers are confused and frustrated with their missions. While environmental laws, \n",
    "no question about it, have shut down logging, particularly in the Pacific Northwest, please give us an opportunity to \n",
    "nurture and care for this resource. To let it burn is huge waste; to let it burn means we lost all the environmental \n",
    "issues that we all deem important; we lost stream bank protection, we lost the resource, we lost wildlife, we lost all \n",
    "of those important issues to all of us in the West for some 250 years. Will this legislation answer all the questions? \n",
    "Of course not. This is a moderate, meager, bipartisan effort to answer some of the problems and some of the forests \n",
    "that are in the worst condition in this Nation. We think that this will give the Forest Service the direction necessary \n",
    "and again, I reiterate, abide by every environmental law in this land. Mr. Chairman, I reserve the balance of my time.\n",
    "\"\"\".replace(\"\\n\", \"\")\n",
    "\n",
    "#print getProperNounReferences(test)\n",
    "#print getNameReferences(test)\n",
    "\n",
    "#['Forest Recovery and Protection Act', 'United Brotherhood of Carpenters and Joiners of America', 'National Association of Counties', 'Society of American Foresters', 'National Association of State Foresters', 'National Association of Professional Forestry Schools', 'National Academy of Sciences', 'Secretary of Agriculture', 'National Environmental Policy Act', 'National Forest', 'The Chief', 'Forest Service', 'National Forest', 'Integrated Scientific Assessment for Ecosystem Management', 'Interior Columbia Basin', 'Appalachian Assessment', 'Southern Appalachians', 'Sierra Nevada Ecosystem Project', 'Forest Service', 'Forest Service', 'The Forest Service', 'Pacific Northwest', 'Forest Service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gensim????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getNumberOfWords(monologue):\n",
    "    return len(monologue.split())\n",
    "\n",
    "def getWordHistogram(wordSet, monologue):\n",
    "    countTuples = [(w, monologue.count(w)) for w in wordSet]\n",
    "    \n",
    "    sortedTuples = sorted(countTuples, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [\"{0}:{1}\".format(w, str(c)) for w, c in sortedTuples]\n",
    " \n",
    "\n",
    "def getLengthOfMonologue(monologues):\n",
    "\n",
    "    lengthOfMonologue = [len(m.split() ) for m in monologues]\n",
    "    return lengthOfMonologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is not implemented - we'd never finish with the latency imposed by this\n",
    "# also, this implementation was sub-par, at the _very_ best\n",
    "def getSentiment(monologues=[]):\n",
    "    sentimentList = []\n",
    "\n",
    "    #for m in monologues:\n",
    "    #    sentimentJson = requests.post('http://sentiment.vivekn.com/api/batch/',data=\"text='\"+m+\"'\")\n",
    "    #    sentiment = json.loads(sentimentJson.text)['label']\n",
    "    #    sentimentList.append(sentiment)\n",
    "    \n",
    "    \n",
    "    jsonPayload = dict(zip(range(len(monologues)), monologues))\n",
    "    \n",
    "    #print sys.getsizeof(jsonPayload)\n",
    "    \n",
    "    sentimentJson = requests.post('http://sentiment.vivekn.com/api/batch/', data=jsonPayload)\n",
    "    #print sentimentJson.headers\n",
    "    \n",
    "    print sentimentJson.json()\n",
    "    #sentiment = json.loads(sentimentJson.text)['label']\n",
    "    \n",
    "    \n",
    "    return sentimentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLeaders(leadersFile):\n",
    "    leaders = pd.read_csv(leadersFile)  #\"/home/kelvin/congression_record/101/houseLeaders.csv\"\n",
    "    leaders[\"First Name\"] = leaders.Name.map(lambda s:s.split()[0])\n",
    "    leaders[\"Last Name\"] = leaders.Name.map(lambda s:s.split()[-1])\n",
    "    return leaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTheRole(leaders, lastName):\n",
    "    try:\n",
    "        if lastName == \"The SPEAKER\":\n",
    "            #lastName = leaders[leaders[\"Role\"] == lastName.capitalize() ][\"Last Name\"].values[0]\n",
    "            role = \"Speaker\"\n",
    "        else:\n",
    "            role = leaders[leaders[\"Last Name\"] == lastName.capitalize().replace(\"Mr.\",\"\").replace(\"Mrs.\",\"\").replace(\"Ms.\",\"\") ].Role.values[0]\n",
    "    except:\n",
    "        role = False\n",
    "    \n",
    "    theRole[lastName] = role\n",
    "    return theRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTermEnd(leaders, lastName):\n",
    "    try:\n",
    "        if lastName == \"The SPEAKER\":\n",
    "            termEnd = leaders[leaders[\"Role\"] == lastName.capitalize() ][\"Term End\"].values[0]\n",
    "        else:\n",
    "            termEnd = leaders[leaders[\"Last Name\"] == lastName.capitalize().replace(\"Mr.\",\"\").replace(\"Mrs.\",\"\").replace(\"Ms.\",\"\") ][\"Term End\"].values[0]\n",
    "    except:\n",
    "        termEnd = False\n",
    "        \n",
    "    theTermEnd[lastName] = termEnd\n",
    "    return theTermEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanNestedWhitespace(text):\n",
    "    return re.sub( '\\s+', ' ', text).strip()\n",
    "\n",
    "def truncateToSection(text):\n",
    "    try:\n",
    "        return text.split(\"____________________\")[0]\n",
    "    except IndexError:\n",
    "        return text\n",
    "    \n",
    "def cleanMonologue(text):\n",
    "    p1 = truncateToSection(text)\n",
    "    return cleanNestedWhitespace(p1)\n",
    "\n",
    "def splitTheList(speakersAndMonologues):\n",
    "    \n",
    "    totalLength = len(speakersAndMonologues)\n",
    "    speakerIndices = None\n",
    "    for n, token in enumerate(speakersAndMonologues):\n",
    "        # If the token starts with a title, its a speaker,\n",
    "        # rather than a monologue\n",
    "        if re.match(\"^(Mr.|Mrs.|Ms.|\\^\\^THE\\^\\^)\", token):\n",
    "            #print n\n",
    "            speakerIndices = range(n, totalLength, 2)\n",
    "            monologueIndices = range(n+1, totalLength, 2)\n",
    "            break\n",
    "            \n",
    "    if speakerIndices is None:\n",
    "        return None, None\n",
    "            \n",
    "    speakers = [speakersAndMonologues[i] for i in speakerIndices]\n",
    "    monologues = [cleanMonologue(speakersAndMonologues[i]) for i in monologueIndices]\n",
    "            \n",
    "    \n",
    "    return speakers, monologues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getState(df, lastName, session):\n",
    "    try:\n",
    "        return df[(df.Last == lastName.title()) & (df.Session == int(session))][\"State\"].values[0]\n",
    "        #return df[(df.Name.str.contains(lastName.title())) & (df.Session == int(session))][\"State\"].values[0]\n",
    "    except IndexError:\n",
    "        try:\n",
    "            # So and so Of someState\n",
    "            if re.match(\"[^\\s]*\\sOF\\s.*\", lastName.upper()):\n",
    "                return re.split(\"[oO][fF]\", lastName.title())[-1].split()[0]\n",
    "            else:\n",
    "                return \"Unknown\"\n",
    "        except:\n",
    "            return \"Unknown\"\n",
    "    \n",
    "def getParty(df, lastName, session):\n",
    "    try:\n",
    "        return df[(df.Last == lastName.title()) & (df.Session == int(session))][\"Party Code\"].values[0]\n",
    "        #return df[(df.Name.str.contains(lastName.title())) & (df.Session == int(session))][\"Party Code\"].values[0]\n",
    "    except IndexError:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "def getFullName(df, name, session):\n",
    "    lastName = \" \".join(name.split()[1:])\n",
    "    try:\n",
    "        nameOrNames = df[(df.Last == lastName.title()) & (df.Session == int(session))][\"Name\"].values\n",
    "        if nameOrNames.shape[0] > 1:\n",
    "            return \"Ambiguous {0}\".format(name)\n",
    "        else:\n",
    "            return nameOrNames[0].replace('\"', \"'\")\n",
    "        #return df[(df.Name.str.contains(lastName.title())) & (df.Session == int(session))][\"Name\"].values[0].replace('\"', \"'\")       \n",
    "    except IndexError:\n",
    "        return name.title().replace('\"', \"'\")\n",
    "    except re.error:\n",
    "        return name.replace('\"', \"'\")\n",
    "    \n",
    "def getSpeaker(df, session):\n",
    "    try:\n",
    "        return df[(df.Role == \"Speaker\") & (df.Session == int(session))][\"Name\"].values[0]\n",
    "    except IndexError:\n",
    "        return \"Session {0} Speaker\".format(str(session))\n",
    "    \n",
    "def getRole(df, lastName, session):\n",
    "    try:\n",
    "        return df[(df.Last == lastName.title()) & (df.Session == int(session))][\"Role\"].values[0]\n",
    "        #return df[(df.Name.str.contains(lastName.title())) & (df.Session == int(session))][\"Role\"].values[0]\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def fullNamesFromReferences(namesInMonologue, congressSession, theBranch):\n",
    "    fullNames = list(namesInMonologue)\n",
    "    for n, name in enumerate(namesInMonologue):\n",
    "        if name.split()[-1] == \"Speaker\":\n",
    "            fullNames[n] = getSpeaker(rosterAndLeaders[theBranch.title()]['Leaders'], congressSession)\n",
    "        \n",
    "        else:\n",
    "            fullNames[n] = getFullName(rosterAndLeaders[theBranch.title()]['Roster'], name, congressSession)\n",
    "        \n",
    "    return fullNames\n",
    "    \n",
    "#getState(rosterAndLeaders['Senate']['Roster'], \"Young\", 101)\n",
    "#getParty(rosterAndLeaders['Senate']['Roster'], \"Young\", 101)\n",
    "#getRole(rosterAndLeaders['House']['Leaders'], \"Foley\", 103)\n",
    "#getSpeaker(rosterAndLeaders['House']['Leaders'], 110)\n",
    "#getFullName(rosterAndLeaders['House']['Roster'], \"Mr. Foley\", 103) -> Hit\n",
    "#getFullName(rosterAndLeaders['Senate']['Roster'], \"Mr. Young\", 103) -> Ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startRegex = re.compile(\"met at (\\d+[: ]?\\d*\\W?(?:o'clock noon|noon|[aApP]\\.[mM]\\.))\")\n",
    "timesRegex = re.compile(\"{time}\\W*([\\d]{4})\")\n",
    "def getTimes(rawText):\n",
    "    match = startRegex.search(rawText)\n",
    "    times = timesRegex.findall(rawText)\n",
    "    \n",
    "    if match is not None:\n",
    "        startTime = match.group(1)\n",
    "        \n",
    "        if len(times) > 0:\n",
    "            lastTime = max(times)\n",
    "        else:\n",
    "            lastTime = \"Unknown\"\n",
    "    elif len(times) > 1:\n",
    "        startTime = min(times)\n",
    "        \n",
    "        lastTime = max(times)\n",
    "    else:\n",
    "        startTime = \"Unknown\"\n",
    "        lastTime = \"Unknown\"\n",
    "        \n",
    "    return startTime, lastTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1015', '2045')\n"
     ]
    }
   ],
   "source": [
    "#test=\"The Senate met at 10 a.m., on the expiration of the recess, and was\"\n",
    "test=\"{time} 1015 \\n\\n\\n\\n\\n\\ {time} 2045 time 3342\"\n",
    "print getTimes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import current_process\n",
    "\n",
    "from csv import writer, QUOTE_ALL, QUOTE_NONE\n",
    "import gzip\n",
    "\n",
    "def writeCsv(monologues, speakers, startTime, lastTime,\n",
    "             monologueWordSets, monologueWordCounts, monologueNameReferences, \n",
    "             sentiment, theBranch, congressSession, theDate, speakerRoles, speakerParties, theTermEnd, \n",
    "             speakerStates, wordHistograms, mentionedProperNouns):\n",
    "    \n",
    "    threadId = current_process().ident\n",
    "    \n",
    "    nodeTypes = [\"Date\", \"Session\", \"Congress\", \"Monologue\", \n",
    "                 \"Person\", \"Party\", \"State\", \"ProperNoun\"]\n",
    "    relTypes = [\"sessionDate\", \"sessionCongress\", \"monologueDate\", \"monologueCongress\",\n",
    "                \"monologueSession\", \"monologueSequence\", \"personParty\", \"personState\",\n",
    "                \"personSpoke\", \"monologueMentions\", \"monologueProperNounMentions\"]\n",
    "    \n",
    "    outFiles = [gzip.open(\"/Users/kyledunn/Desktop/congressionalRecord/normalized/{0}-{1}.csv.gz\".format(t, threadId), 'a') for t in nodeTypes + relTypes]\n",
    "    csvWriters = [writer(f, delimiter='|', quotechar='', quoting=QUOTE_NONE, doublequote=False) for f in outFiles]\n",
    "\n",
    "    writers = dict(zip(nodeTypes + relTypes, csvWriters))\n",
    "    #writers['Date'].writerow(['Spam'] * 5 + ['Baked Beans'])\n",
    "    \n",
    "    \"\"\"\n",
    "    http://neo4j.com/docs/stable/cypherdoc-importing-csv-files-with-cypher.html\n",
    "    http://stackoverflow.com/questions/31639855/avoid-processing-duplicate-data-when-csv-importing-via-cypher\n",
    "    \n",
    "    LOAD CSV FROM \"...\" AS row\n",
    "    OPTIONAL MATCH (f:MyLabel {id:row.uniqueId})\n",
    "    WHERE f IS NULL\n",
    "    MERGE (f:MyLabel {id:row.uniqueId})\n",
    "    ON CREATE SET f....\n",
    "    WITH f,row\n",
    "    MATCH (otherNode:OtherLabel {id : row.otherNodeId})\n",
    "    MERGE (f) -[:REL1] -> (otherNode)\n",
    "    \"\"\";\n",
    "    \n",
    "    \n",
    "    #graphDb = py2neo.Graph(\"http://{u}:{p}@{h}:7474/db/data\".format(u=user, p=password, h=host))\n",
    "    \n",
    "    # Do the thing all in one transaction\n",
    "    #tx = graphDb.cypher.begin()\n",
    "  \n",
    "    \"\"\"\n",
    "    USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///../../../../Desktop/congressionalRecord/pipe\" as row \n",
    "    FIELDTERMINATOR '|' \n",
    "    CREATE (n:Date {date:row[0], branch:row[1], startTime:row[2], stopTime:row[3]})\n",
    "    \"\"\";\n",
    "\n",
    "    #dateNode = MergeNode(\"Date\").set(date=theDate, startTime=startTime, stopTime=lastTime)\n",
    "    #tx.append(dateNode)\n",
    "    writers['Date'].writerow([theDate, theBranch, startTime, lastTime])\n",
    " \n",
    "    \"\"\"\n",
    "    USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///../../../../Desktop/congressionalRecord/pipe\" as row \n",
    "    FIELDTERMINATOR '|' \n",
    "    CREATE (:Session { label:row[0]} )\n",
    "    \"\"\";\n",
    "\n",
    "    thisBranchSession = \"{0} Session\".format(theBranch)\n",
    "    #writers['Session'].writerow([thisBranchSession])\n",
    "\n",
    "    \"\"\"\n",
    "    USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///../../../../Desktop/congressionalRecord/pipe\" as row \n",
    "    FIELDTERMINATOR '|' \n",
    "    CREATE (:Congress { label:row[0]} )\n",
    "    \"\"\";\n",
    "    \n",
    "    #congressYearNode = MergeNode(\"Congress {0}\".format(congressSession))\n",
    "    #tx.append(congressYearNode)\n",
    "    thisCongress = \"Congress {0}\".format(congressSession)\n",
    "    #writers['Congress'].writerow([thisCongress])\n",
    "    \n",
    "    #statement = \"\"\"\n",
    "    #MATCH (sp:{0}) \n",
    "    #MATCH (da:Date {{date: {{d}}}}) \n",
    "    #CREATE (sp)-[:`ON DATE`]->(da)\n",
    "    #\"\"\".format(\"`{0} Session`\".format(theBranch))\n",
    "    #tx.append(statement, {\"d\": theDate})\n",
    " \n",
    "    \"\"\"\n",
    "    USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///Users/kyledunn/Desktop/congressionalRecord/pipe\" as row \n",
    "    FIELDTERMINATOR '|' \n",
    "    MATCH (br:Session)\n",
    "    MATCH (da:Date {date:row[2]})\n",
    "    WHERE br.label = row[0]\n",
    "    CREATE (br)-[:`ON DATE`]->(da)\n",
    "    \"\"\";\n",
    "\n",
    "    writers[\"sessionDate\"].writerow([thisBranchSession, \"ON DATE\", theDate])\n",
    "    \n",
    "    #tx.append(\"\"\"\n",
    "    #MATCH (s:{s})\n",
    "    #MATCH (d:{d})\n",
    "    #CREATE (s)-[:`PART OF`]->(d)\n",
    "    #\"\"\".format(s=\"`{0} Session`\".format(theBranch) , \n",
    "    #           d=\"`Congress {0}`\".format(congressSession)))\n",
    "\n",
    "    \"\"\"\n",
    "    USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///Users/kyledunn/Desktop/congressionalRecord/pipe\" as row \n",
    "    FIELDTERMINATOR '|' \n",
    "    MATCH (br:Session)\n",
    "    MATCH (cg:Congress)\n",
    "    WHERE br.label = row[0] AND cg.id = row[2]\n",
    "    CREATE (br)-[:`PART OF`]->(cg)\n",
    "    \"\"\";\n",
    "    \n",
    "    writers[\"sessionCongress\"].writerow([thisBranchSession, \"PART OF\", thisCongress])\n",
    "    \n",
    "    #tx.commit()\n",
    "    \n",
    "    previousMonologueId = None\n",
    "    \n",
    "    for index, m in enumerate(monologues):\n",
    "        \n",
    "        #tx = graphDb.cypher.begin()\n",
    "        \n",
    "        monologueId = \"-\".join([theDate, theBranch, str(congressSession), 'monologue', str(index)])\n",
    "        \"\"\"\n",
    "        monologueNode = CreateNode(\"Monologue\").set(id=monologueId,\n",
    "                                                    speaker=speakers[index], \n",
    "                                                    branch=theBranch, \n",
    "                                                    congressionalYear=congressSession,\n",
    "                                                    date=theDate,\n",
    "                                                    text=m,\n",
    "                                                    numWords=monologueWordCounts[index],\n",
    "                                                    wordSet=monologueWordSets[index],\n",
    "                                                    wordHistogram=wordHistograms[index],\n",
    "                                                    properNouns=mentionedProperNouns[index],\n",
    "                                                    party=speakerParties[index],\n",
    "                                                    role=speakerRoles[index])\n",
    "        tx.append(monologueNode)\n",
    "        \"\"\";\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/Monologue-123145338265600.csv.gz\" AS row\n",
    "        CREATE (m:Monologue {id:row[0], \n",
    "                             speaker:row[1], \n",
    "                             branch:row[2], \n",
    "                             congressionalYear:row[3],\n",
    "                             date:row[4],\n",
    "                             text:row[5],\n",
    "                             numWords:row[6],\n",
    "                             wordSet:row[7],\n",
    "                             wordHistogram:row[8],\n",
    "                             properNouns:row[9],\n",
    "                             party:row[10],\n",
    "                             role:row[11]})\n",
    "        \"\"\";\n",
    "        \n",
    "        monologueFields = [monologueId, speakers[index], theBranch, congressSession, \n",
    "                           theDate, \" {0} \".format(m.replace('\"', r'\\\"')), \n",
    "                           monologueWordCounts[index], monologueWordSets[index], \n",
    "                           wordHistograms[index], mentionedProperNouns[index], speakerParties[index],\n",
    "                           speakerRoles[index]]\n",
    "        writers[\"Monologue\"].writerow(monologueFields)\n",
    "        \n",
    "        #tx.append(\"\"\"\n",
    "        #MATCH (m:Monologue {id: {i}})\n",
    "        #MATCH (d:Date {date: {d}})\n",
    "        #CREATE (m)-[:`ON DATE`]->(d)\"\"\", {\"i\": monologueId, \"d\": theDate})\n",
    "        \n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT ON (m:Monologue) ASSERT m.id IS UNIQUE\n",
    "        USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///Users/kyledunn/Desktop/congressionalRecord/pipe\" as row \n",
    "        FIELDTERMINATOR '|' \n",
    "        MATCH (m:Monologue {id:row[0]})\n",
    "        MATCH (da:Date {date:row[2]})\n",
    "        CREATE (m)-[:`ON DATE`]->(da)\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"monologueDate\"].writerow([monologueId, \"ON DATE\", theDate])\n",
    "\n",
    "        #tx.append(\"\"\"\n",
    "        #MATCH (m:Monologue {{id: \"{i}\"}})\n",
    "        #MATCH (s:{s})\n",
    "        #CREATE (m)-[:`PART OF`]->(s)\n",
    "        #\"\"\".format(i=monologueId , s=\"`Congress {0}`\".format(congressSession)))\n",
    "        #, {\"i\": monologueId, \"s\": })\n",
    "\n",
    "        \"\"\"\n",
    "        USING PERIODIC COMMIT 1000 LOAD CSV FROM \"file:///Users/kyledunn/Desktop/congressionalRecord/pipe\" as row \n",
    "        FIELDTERMINATOR '|' \n",
    "        MATCH (m:Monologue {id:row[0]})\n",
    "        MATCH (cg:Congress)\n",
    "        WHERE cg.id = row[2]\n",
    "        CREATE (m)-[:`row[1]`]->(cg)\n",
    "        \"\"\";\n",
    "\n",
    "        writers[\"monologueCongress\"].writerow([monologueId, \"PART OF\", thisCongress])\n",
    "\n",
    "        #tx.append(\"\"\"\n",
    "        #MATCH (m:Monologue {{id: \"{i}\"}})\n",
    "        #MATCH (s:{s})\n",
    "        #CREATE (m)-[:`PART OF`]->(s)\n",
    "        #\"\"\".format(i=monologueId , s=\"`{0} Session`\".format(theBranch)))         \n",
    "        #, {\"i\": monologueId, \"s\": })\n",
    "\n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/monologueSession-*.csv.gz\" AS row\n",
    "        MATCH (m:Monologue {id:row[0]})\n",
    "        MATCH (bs:`row[2]`)\n",
    "        CREATE (m)-[:`row[1]`]->(bs)\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"monologueSession\"].writerow([monologueId, \"PART OF\", thisBranchSession])\n",
    "\n",
    "        \n",
    "        if previousMonologueId is not None:\n",
    "            #tx.append(\"\"\"\n",
    "            #MATCH (m:Monologue {id: {i}})\n",
    "            #MATCH (n:Monologue {id: {j}})\n",
    "            #CREATE (m)-[:`SAID BEFORE`]->(n)\"\"\", {\"i\": previousMonologueId, \"j\": monologueId})\n",
    "            \n",
    "            \"\"\"\n",
    "            LOAD CSV FROM \"file:///path/to/monologueSequence-*.csv.gz\" AS row\n",
    "            MATCH (m:Monologue {id:row[0]})\n",
    "            MATCH (n:Monologue {id:row[2]})\n",
    "            CREATE (m)-[:`row[1]`]->(n)\n",
    "            \"\"\";\n",
    "            \n",
    "            writers[\"monologueSequence\"].writerow([previousMonologueId, \"SAID BEFORE\", monologueId])\n",
    "\n",
    "        #tx.append(MergeNode(\"Person\", \"name\", speakers[index]).set(party=speakerParties[index], \n",
    "        #                                                           role=speakerRoles[index],\n",
    "        #                                                           state=speakerStates[index]))\n",
    "        \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/Person-*.csv.gz\" AS row\n",
    "        MATCH (sp:Person {name:row[0]})\n",
    "        WHERE sp.party IS NULL OR sp.state IS NULL\n",
    "        MERGE (sp:Person {name:row[0], party:row[1], role:row[2], state:row[3]})\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"Person\"].writerow([speakers[index], speakerParties[index], \n",
    "                                    speakerRoles[index], speakerStates[index]])\n",
    "\n",
    "        \n",
    "        #tx.append(MergeNode(\"Party\", \"name\", speakerParties[index]))\n",
    "        \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/Party-*.csv.gz\" AS row\n",
    "        MERGE (st:Party {name:row[0]})\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"Party\"].writerow([speakerParties[index]])\n",
    "        \n",
    "        #tx.append(\"\"\"\n",
    "        #MATCH (speaker:Person {{name: \"{s}\"}})\n",
    "        #MATCH (party:Party {{name: \"{p}\"}})\n",
    "        #MERGE (speaker)-[r:`MEMBER OF`]->(party) \n",
    "        #RETURN r\n",
    "        #\"\"\".format(s=speakers[index], p=speakerParties[index]))\n",
    "        \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/personParty-*.csv.gz\" AS row\n",
    "        MATCH (sp:Person {name:row[0]})\n",
    "        MATCH (pa:Party {name:row[2]})\n",
    "        CREATE (sp)-[:`row[1]`]->(pa)\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"personParty\"].writerow([speakers[index], \"MEMBER OF\",\n",
    "                                         speakerParties[index]])\n",
    "        \n",
    "        #tx.append(MergeNode(\"State\", \"name\", speakerStates[index].strip()))\n",
    "       \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/State-*.csv.gz\" AS row\n",
    "        MERGE (st:State {name:row[0]})\n",
    "        \"\"\";\n",
    "    \n",
    "        writers[\"State\"].writerow([speakerStates[index].strip()])\n",
    "        \n",
    "        #tx.append(\"\"\"\n",
    "        #MATCH (speaker:Person {{name: \"{s}\"}})\n",
    "        #MATCH (state:State {{name: \"{p}\"}})\n",
    "        #MERGE (speaker)-[r:FROM]->(state)\n",
    "        #RETURN r\n",
    "        #\"\"\".format(s=speakers[index], p=speakerStates[index].strip()))\n",
    "        \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/personState-*.csv.gz\" AS row\n",
    "        MATCH (sp:Person {name:row[0]})\n",
    "        MATCH (st:State {name:row[2]})\n",
    "        CREATE (sp)-[:`row[1]`]->(st)\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"personState\"].writerow([speakers[index], \"FROM\",\n",
    "                                         speakerStates[index].strip()])\n",
    "        \n",
    "        #tx.append(\"\"\"\n",
    "        #MATCH (n:Person {{name: \"{n}\"}})\n",
    "        #MATCH (m:Monologue {{id: \"{i}\"}})\n",
    "        #CREATE (n)-[r:SPOKE]->(m) \n",
    "        #RETURN r\n",
    "        #\"\"\".format(n=speakers[index], i=monologueId))\n",
    "        #, {\"n\": speakers[index], \"i\": monologueId})\n",
    "        \n",
    "        \"\"\"\n",
    "        LOAD CSV FROM \"file:///path/to/personSpoke-*.csv.gz\" AS row\n",
    "        MATCH (sp:Person {name:row[0]})\n",
    "        MATCH (m:Monologue {id:row[2]})\n",
    "        CREATE (sp)-[:`row[1]`]->(m)\n",
    "        \"\"\";\n",
    "        \n",
    "        writers[\"personSpoke\"].writerow([speakers[index], \"SPOKE\",  monologueId])\n",
    "\n",
    "        #tx.commit()\n",
    "        #tx = graphDb.cypher.begin()\n",
    "        \n",
    "        for p in monologueNameReferences[index]:\n",
    "            #tx.append(\"\"\"MERGE (n:Person {name: {N}}) RETURN n\"\"\", {\"N\": m})\n",
    "            \n",
    "            writers[\"Person\"].writerow([p, None, None, None])\n",
    "\n",
    "            #tx.append(\"\"\"\n",
    "            #MATCH (speaker:Person {name: {s}})\n",
    "            #MATCH (person:Person {name: {p}})\n",
    "            #MERGE (speaker)-[r:`MENTIONED PERSON`]->(person)\n",
    "            #ON CREATE set r.count = 1, r.monologueIds = [{m}]\n",
    "            #ON MATCH set r.count = r.count + 1, r.monologueIds = r.monologueIds + {m}\n",
    "            #RETURN r \"\"\", {\"s\": speakers[index], \"p\": m, \"m\": monologueId});\n",
    "            #WITH speaker.name = \"{s}\" AND person.name = \"{p}\"\n",
    "        \n",
    "            \"\"\"\n",
    "            LOAD CSV FROM \"file:///path/to/monologueMentions-*.csv.gz\" AS row\n",
    "            MATCH (sp:Person {name:row[0]})\n",
    "            MATCH (p:Person {name:row[2]})\n",
    "            MERGE (sp)-[r:`row[1]`]->(p)\n",
    "            ON CREATE SET r.count = 1, r.monologueIds = [row[3]]\n",
    "            ON MATCH SET r.count = r.count + 1, r.monologueIds = r.monologueIds + row[3]\n",
    "            \"\"\";\n",
    "        \n",
    "            # TODO play the match, create/merge trick above on load \n",
    "            writers[\"monologueMentions\"].writerow([speakers[index], \"MENTIONED PERSON\",\n",
    "                                                   p, monologueId])\n",
    "            \n",
    "        #tx.commit()\n",
    "        #tx = graphDb.cypher.begin()\n",
    "            \n",
    "        for noun in mentionedProperNouns[index]:\n",
    "            #tx.append(\"\"\"MERGE (n:`Proper Noun` {name: {N}}) RETURN n\"\"\", {\"N\": noun})\n",
    "           \n",
    "            \"\"\"\n",
    "            1)\n",
    "            zcat uniqueProperNoun.csv > pipe\n",
    "            2)\n",
    "            LOAD CSV FROM \"file:////Users/kyledunn/Desktop/congressionalRecord/pipe\" AS row FIELDTERMINATOR '|'\n",
    "            CREATE (:ProperNoun { label:row[0] })\n",
    "            \"\"\";\n",
    "        \n",
    "            writers[\"ProperNoun\"].writerow([noun])\n",
    "\n",
    "            #tx.append(\n",
    "            #\"\"\"\n",
    "            #MATCH (speaker:Person {name: {s}})\n",
    "            #MATCH (noun:`Proper Noun` {name: {n}})\n",
    "            #MERGE (speaker)-[r:`MENTIONED PROPER NOUN`]->(noun)\n",
    "            #ON CREATE set r.count = 1, r.monologueIds = [{m}]\n",
    "            #ON MATCH set r.count = r.count + 1, r.monologueIds = r.monologueIds + {m}\n",
    "            #RETURN r\n",
    "            #\"\"\", {\"s\": speakers[index], \"n\": noun, \"m\": monologueId});\n",
    "            #WITH speaker.name = \"{s}\" AND noun.name = \"{n}\"\n",
    "            \n",
    "            \"\"\"\n",
    "            LOAD CSV FROM \"file:///path/to/monologueProperNounMentions-*.csv.gz\" AS row\n",
    "            MATCH (sp:Person {name:row[0]})\n",
    "            MATCH (n:`Proper Noun` {name:row[2]})\n",
    "            MERGE (sp)-[r:`row[1]`]->(n)\n",
    "            ON CREATE SET r.count = 1, r.monologueIds = [row[3]]\n",
    "            ON MATCH SET r.count = r.count + 1, r.monologueIds = r.monologueIds + row[3]\n",
    "            \"\"\";\n",
    "            \n",
    "            # TODO play the match, create/merge trick above on load\n",
    "            writers[\"monologueProperNounMentions\"].writerow([speakers[index], \"MENTIONED PROPER NOUN\",\n",
    "                                                             noun, monologueId])\n",
    "        \n",
    "        # Update the last monologue for context linkage\n",
    "        previousMonologueId = monologueId\n",
    "        \n",
    "    #tx.commit()\n",
    "    for f in outFiles:\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np        \n",
    "\n",
    "yearLut = dict()\n",
    "for n, y in enumerate(np.arange(1993, 2017, 2)):\n",
    "    yearLut[y] = 103+n\n",
    "    yearLut[y+1] = 103+n\n",
    "    \n",
    "#print yearLut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    root = \"/Users/kdunn/Dropbox/workspace/python/congressionalRecord/\"\n",
    "    rosterAndLeaders = dict()\n",
    "    rosterAndLeaders['House'] = dict({\"Roster\": pd.read_csv(root + \"hRoster.csv\" ), \n",
    "                                     \"Leaders\": pd.read_csv(root + \"hLeaders.csv\" )})\n",
    "\n",
    "    rosterAndLeaders['Senate'] = dict({\"Roster\": pd.read_csv(root + \"sRoster.csv\" ),\n",
    "                                      \"Leaders\": pd.read_csv(root + \"sLeaders.csv\" )})\n",
    "\n",
    "except IOError:\n",
    "    root = \"/Users/kyledunn/Dropbox/workspace/python/congressionalRecord/\"\n",
    "    rosterAndLeaders = dict()\n",
    "    rosterAndLeaders['House'] = dict({\"Roster\": pd.read_csv(root + \"hRoster.csv\" ), \n",
    "                                     \"Leaders\": pd.read_csv(root + \"hLeaders.csv\" )})\n",
    "    \n",
    "    rosterAndLeaders['House']['Roster']['Last'] = rosterAndLeaders['House']['Roster'].Name.map(lambda s: str(s).strip().split()[-1])\n",
    "    rosterAndLeaders['House']['Leaders']['Last'] = rosterAndLeaders['House']['Leaders'].Name.map(lambda s: str(s).strip().split()[-1])\n",
    "\n",
    "    \n",
    "    rosterAndLeaders['Senate'] = dict({\"Roster\": pd.read_csv(root + \"sRoster.csv\" ),\n",
    "                                      \"Leaders\": pd.read_csv(root + \"sLeaders.csv\" )})\n",
    "    \n",
    "    rosterAndLeaders['Senate']['Roster']['Last'] = rosterAndLeaders['Senate']['Roster'].Name.map(lambda s: str(s).strip().split()[-1])\n",
    "    rosterAndLeaders['Senate']['Leaders']['Last'] = rosterAndLeaders['Senate']['Leaders'].Name.map(lambda s: str(s).strip().split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from py2neo.core import Unauthorized\n",
    "#from py2neo.packages.httpstream import SocketError\n",
    "\n",
    "def process(theRecord):\n",
    "    \n",
    "    #print \"Filename\", theRecord\n",
    "    path = theRecord.split(\"/\")\n",
    "    \n",
    "    theFilename = path[-1]\n",
    "    theBranch = path[-3].title()\n",
    "    \n",
    "    theDate = \"\".join(theFilename.split('.')[0].split(\"-\")) #[-3:])\n",
    "    \n",
    "    try:\n",
    "        congressSession = yearLut[int(theDate[:4])]\n",
    "    except KeyError:\n",
    "        print \"Failed to lookup\", theDate\n",
    "        return\n",
    "    \n",
    "    #print \"Loading congress\", congressSession, \"date:\", theDate\n",
    "    \n",
    "    theSpeaker = \"The SPEAKER\"\n",
    "    if theBranch == \"House\":\n",
    "        theSpeaker = getSpeaker(rosterAndLeaders['House']['Leaders'], congressSession)\n",
    "    \n",
    "    speakersAndMonologues, rawText = getMonologues(theRecord, theSpeaker)\n",
    "    \n",
    "    startTime, lastTime = getTimes(rawText)\n",
    "    \n",
    "    speakers, monologues = splitTheList(speakersAndMonologues)\n",
    "    \n",
    "    if speakers is None or monologues is None:\n",
    "        print \"Bad splits for\", theBranch, theDate, '-', theFilename\n",
    "        return\n",
    "    \n",
    "    monologueWordSets = map(getSetOfWords, monologues)\n",
    "    \n",
    "    monologueWordHistograms = map(getWordHistogram, monologueWordSets, monologues)\n",
    "    \n",
    "    monologueWordCounts = map(getNumberOfWords, monologues)\n",
    "    \n",
    "    monologueNameReferences = map(getNameReferences, monologues)\n",
    "    \n",
    "    # Takes a *long* time\n",
    "    #sentiment = getSentiment(monologues)\n",
    "    #sentiment = []\n",
    "    \n",
    "    speakerRoles = map(lambda s: getRole(rosterAndLeaders[theBranch.title()]['Leaders'], \n",
    "                                         \" \".join(s.split()[1:]), \n",
    "                                         congressSession), speakers)\n",
    "    \n",
    "    speakerParties = map(lambda s: getParty(rosterAndLeaders[theBranch.title()]['Roster'],\n",
    "                                            \" \".join(s.split()[1:]),\n",
    "                                            congressSession), speakers)\n",
    "    \n",
    "    speakerStates = map(lambda s: getState(rosterAndLeaders[theBranch.title()]['Roster'],\n",
    "                                           \" \".join(s.split()[1:]),\n",
    "                                           congressSession), speakers)\n",
    "\n",
    "    speakerFullNames = map(lambda s: getFullName(rosterAndLeaders[theBranch.title()]['Roster'], s,\n",
    "                                                 congressSession), speakers)\n",
    "    \n",
    "    mentionedFullNames = map(lambda l: fullNamesFromReferences(l, congressSession, theBranch), monologueNameReferences)\n",
    "    \n",
    "    mentionedProperNouns = map(getProperNounReferences, monologues)\n",
    "    \n",
    "    if True:\n",
    "        try:\n",
    "            writeCsv(monologues, speakerFullNames, startTime, lastTime,\n",
    "                     monologueWordSets, monologueWordCounts, mentionedFullNames, \n",
    "                     None, theBranch,  congressSession, theDate, speakerRoles, speakerParties, \n",
    "                     None, speakerStates, monologueWordHistograms, mentionedProperNouns)\n",
    "        except:\n",
    "            print theBranch, theDate, \"shit the bed\", \"-\", theFilename\n",
    "            pass\n",
    "        #print \"Wrote\", theDate, \"to CSV.GZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from accelerate import profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "numThreads = 4\n",
    "path = \"/Users/k*dunn/Desktop/congressionalRecord/*/Merged/*.txt\"\n",
    "\n",
    "pool = ThreadPool(numThreads)\n",
    "\n",
    "pool.map(process, glob.glob(path));\n",
    "\n",
    "# Wall time: 9h 35min 11s for everything @24 threads\n",
    "# Wall time: 9h 34min 56s for everything @16\n",
    "# Wall time: 9h 23min 2s for everything @6\n",
    "# Wall time: 9h 26min 30s @4\n",
    "\n",
    "#map(process, glob.glob(path)[:2]);\n",
    "\n",
    "pool.close() \n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "p = profiler.Profile()\n",
    "\n",
    "path = \"/Users/k*dunn/Desktop/congressionalRecord/HOUSE/Merged/*.txt\"\n",
    "\n",
    "p.run('map(process, glob.glob(path)[:2])')\n",
    "\n",
    "profiler.plot(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "monologueHeader.csv:\n",
    "id:ID,speaker,branch,congressionalYear,date,text,numWords,wordSet,wordHistogram,properNouns,party,role\n",
    "\n",
    "./bin/neo4j-import \\\n",
    "--into /Users/kyledunn/neo4j-community-2.2.1/data/graph.db \\\n",
    "--nodes:Monologue \"monologueHeader.csv,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145405571072.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145409777664.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145413984256.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145418190848.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145422397440.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145426604032.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145430810624.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145435017216.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145439223808.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145443430400.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145447636992.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145451843584.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145456050176.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145460256768.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145464463360.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145468669952.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145472876544.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145477083136.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145481289728.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145485496320.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145489702912.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145493909504.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145498116096.csv.gz,/Users/kyledunn/Desktop/congressionalRecord/normalized/Monologue-123145502322688.csv.gz\" \\\n",
    "--skip-duplicate-nodes \\\n",
    "--bad-tolerance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
